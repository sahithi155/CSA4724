# Q8_mlp_varied_inputs_lr_activation.py
# Run in Google Colab as one cell.

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons, make_circles, load_iris
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

# Utility: plot 2D decision boundary
def plot_decision_boundary(clf, X, y, ax=None, title='Decision boundary'):
    if ax is None:
        fig, ax = plt.subplots(figsize=(6,5))
    x_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5
    y_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),
                         np.linspace(y_min, y_max, 300))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    ax.contourf(xx, yy, Z, alpha=0.3)
    ax.scatter(X[:,0], X[:,1], c=y, edgecolor='k', s=40)
    ax.set_title(title)

# Datasets to try
datasets = {
    'moons': make_moons(n_samples=1000, noise=0.2, random_state=1),
    'circles': make_circles(n_samples=1000, noise=0.15, factor=0.5, random_state=1),
    'iris': load_iris(return_X_y=True)
}

# Hyperparameters to sweep
learning_rates = [0.001, 0.01, 0.1]
activations = ['relu', 'tanh', 'logistic']  # common playground choices
hidden_layer = (50,)  # default small network (can be changed)

# Run experiments
for name, data in datasets.items():
    X, y = data
    print(f"\n=== Dataset: {name} ===")
    # If iris, we'll evaluate pairs of features (to simulate "different inputs")
    if name == 'iris':
        feature_pairs = [(0,1),(0,2),(0,3),(1,2),(1,3),(2,3)]
        for (i,j) in feature_pairs:
            Xpair = X[:, [i,j]]
            X_train, X_test, y_train, y_test = train_test_split(Xpair, y, test_size=0.3, random_state=42, stratify=y)
            scaler = StandardScaler().fit(X_train)
            X_train_s = scaler.transform(X_train)
            X_test_s = scaler.transform(X_test)
            print(f"\nIris features {i} & {j} -> train/test sizes: {X_train.shape}/{X_test.shape}")
            for lr in learning_rates:
                for act in activations:
                    clf = MLPClassifier(hidden_layer_sizes=hidden_layer, activation=act, learning_rate_init=lr,
                                        max_iter=1000, random_state=1)
                    clf.fit(X_train_s, y_train)
                    ypred = clf.predict(X_test_s)
                    acc = accuracy_score(y_test, ypred)
                    print(f"act={act:7s} lr={lr:<6} acc={acc:.4f}")
            # plot decision boundary for one representative setting
            clf = MLPClassifier(hidden_layer_sizes=hidden_layer, activation='relu', learning_rate_init=0.01,
                                max_iter=1000, random_state=1)
            clf.fit(X_train_s, y_train)
            fig, ax = plt.subplots(figsize=(5,4))
            plot_decision_boundary(clf, X_test_s, y_test, ax=ax, title=f"Iris features {i}&{j} (relu, lr=0.01)")
            plt.show()
    else:
        # 2D datasets: evaluate directly and plot
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
        scaler = StandardScaler().fit(X_train)
        X_train_s = scaler.transform(X_train)
        X_test_s = scaler.transform(X_test)
        for lr in learning_rates:
            for act in activations:
                clf = MLPClassifier(hidden_layer_sizes=hidden_layer, activation=act, learning_rate_init=lr,
                                    max_iter=1000, random_state=1)
                clf.fit(X_train_s, y_train)
                ypred = clf.predict(X_test_s)
                acc = accuracy_score(y_test, ypred)
                print(f"{name:7s} act={act:7s} lr={lr:<6} acc={acc:.4f}")
        # Plot sample decision boundaries
        fig, axes = plt.subplots(1, len(activations), figsize=(15,4))
        for ax, act in zip(axes, activations):
            clf = MLPClassifier(hidden_layer_sizes=hidden_layer, activation=act, learning_rate_init=0.01,
                                max_iter=1000, random_state=1)
            clf.fit(X_train_s, y_train)
            plot_decision_boundary(clf, X_test_s, y_test, ax=ax, title=f"{name} - {act}")
        plt.show()
