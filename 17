# cnn_binary_softmax.py
# Purpose: CNN binary classification (MNIST 0 vs 1) using softmax (2-unit) final layer.
# Run in Colab/Jupyter. Requires: tensorflow, numpy, matplotlib

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
np.random.seed(42)
tf.random.set_seed(42)

# 1) Load MNIST, select only digits 0 and 1 (binary)
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
X = np.concatenate([x_train, x_test])
Y = np.concatenate([y_train, y_test])

mask = (Y == 0) | (Y == 1)
X = X[mask]
Y = Y[mask]

# normalize and add channel
X = X.astype('float32') / 255.0
X = X[..., np.newaxis]   # shape (N,28,28,1)
Y_cat = to_categorical(Y, num_classes=2)

# split
X_tr, X_val, y_tr, y_val = train_test_split(X, Y_cat, test_size=0.2, random_state=42, stratify=Y)

# 2) Build simple CNN with softmax final
def make_cnn():
    model = models.Sequential([
        layers.Conv2D(16, (3,3), activation='relu', input_shape=(28,28,1)),
        layers.MaxPooling2D(),
        layers.Conv2D(32, (3,3), activation='relu'),
        layers.MaxPooling2D(),
        layers.Flatten(),
        layers.Dense(64, activation='relu'),
        layers.Dense(2, activation='softmax')   # softmax for 2 classes
    ])
    return model

model = make_cnn()
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# 3) Train
history = model.fit(X_tr, y_tr, validation_data=(X_val, y_val), epochs=8, batch_size=64, verbose=1)

# 4) Evaluate
loss, acc = model.evaluate(X_val, y_val, verbose=0)
print(f"Validation accuracy: {acc:.4f}")

# 5) Example predictions and confusion matrix
y_pred = model.predict(X_val)
y_pred_labels = np.argmax(y_pred, axis=1)
y_true_labels = np.argmax(y_val, axis=1)

from sklearn.metrics import confusion_matrix, classification_report
print("Confusion matrix:\n", confusion_matrix(y_true_labels, y_pred_labels))
print("Classification report:\n", classification_report(y_true_labels, y_pred_labels))

# Plot training curve
plt.plot(history.history['accuracy'], label='train_acc')
plt.plot(history.history['val_accuracy'], label='val_acc')
plt.legend(); plt.title('Accuracy'); plt.show()
