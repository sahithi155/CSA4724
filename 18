# cnn_hyperparam_grid.py
# Purpose: vary batch_size, optimizer, activation, learning_rate for CNN (MNIST 0 vs 1)
# Run in Colab/Jupyter. Requires: tensorflow, numpy

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import itertools
np.random.seed(42)
tf.random.set_seed(42)

# Prepare MNIST 0 vs 1
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
X = np.concatenate([x_train, x_test])
Y = np.concatenate([y_train, y_test])
mask = (Y == 0) | (Y == 1)
X = X[mask].astype('float32') / 255.0
X = X[..., np.newaxis]
Y = Y[mask]
Y_cat = to_categorical(Y, num_classes=2)
X_tr, X_val, y_tr, y_val = train_test_split(X, Y_cat, test_size=0.2, random_state=42, stratify=Y)

# Hyperparameter grid (small)
batch_sizes = [32, 128]
optimizers_cfg = [('adam', None), ('sgd', 0.01)]
activations = ['relu', 'tanh']
learning_rates = [0.001, 0.01]  # used for sgd or adam override

def make_model(activation='relu'):
    m = models.Sequential([
        layers.Conv2D(16, (3,3), activation=activation, input_shape=(28,28,1)),
        layers.MaxPooling2D(),
        layers.Conv2D(32, (3,3), activation=activation),
        layers.MaxPooling2D(),
        layers.Flatten(),
        layers.Dense(64, activation=activation),
        layers.Dense(2, activation='softmax')
    ])
    return m

results = []
for batch_size, (opt_name, default_lr), activation, lr in itertools.product(batch_sizes, optimizers_cfg, activations, learning_rates):
    # if optimizer is sgd, use provided lr; for adam, use lr variable too
    if opt_name == 'sgd':
        opt = optimizers.SGD(learning_rate=lr)
    else:
        opt = optimizers.Adam(learning_rate=lr)
    model = make_model(activation=activation)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    # Train for few epochs to compare
    hist = model.fit(X_tr, y_tr, validation_data=(X_val, y_val), epochs=6, batch_size=batch_size, verbose=0)
    val_acc = hist.history['val_accuracy'][-1]
    print(f"bs={batch_size}, opt={opt_name}, act={activation}, lr={lr} -> val_acc={val_acc:.4f}")
    results.append((batch_size, opt_name, activation, lr, val_acc))

# Summarize top 5
results_sorted = sorted(results, key=lambda x: x[4], reverse=True)
print("\nTop results (val_acc):")
for r in results_sorted[:6]:
    print(r)
